{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Emotion Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Install Miniconda from their website\n",
    "* Put Conda on PATH during the install\n",
    "* Update conda - Terminal Command: conda update conda\n",
    "* Clone the Github repository (send Tim or I your github username if you havenâ€™t yet).\n",
    "* Load environment.yaml into conda environment (this will take a while to download) - Terminal Command: conda env create -n mus2vid -f environment.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Music and Emotion data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jamendo/DEAM dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process wav files and extract features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider using MIDI data for this as well as librosa, or just librosa features that Palm found in a paper\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pickle\n",
    "import lzma\n",
    "\n",
    "\n",
    "def get_matched_librosa(audio, genres):\n",
    "    \"\"\"\n",
    "    Takes in a list of audio filepaths and creates a dataframe column of mel spectrograms with matched\n",
    "    genres, similar to the midi object dataframe\n",
    "    \"\"\"\n",
    "    ys = []\n",
    "    srs = []\n",
    "\n",
    "    iters = 0\n",
    "    for file in audio:\n",
    "        print(str(iters) + \"/\" + str(len(audio)))\n",
    "        iters += 1\n",
    "\n",
    "        y, sr = librosa.load(\"wav_clips/\" + file)\n",
    "        ys.append(y)\n",
    "        srs.append(srs)\n",
    "\n",
    "    df = pd.DataFrame({'y': ys, 'sr': srs, 'genre': genres})\n",
    "    \n",
    "    return df\n",
    "\n",
    "matched_librosa_df = get_matched_librosa(audio_clips, clip_genres)\n",
    "\n",
    "\n",
    "\n",
    "with lzma.open(\"matched_librosa.xz\", \"wb\") as f:\n",
    "    pickle.dump(matched_librosa_df, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pickle\n",
    "import lzma\n",
    "\n",
    "def get_features(y, sr):\n",
    "    mfcc = librosa.feature.mfcc(y=y,sr=sr)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y,sr=sr)\n",
    "    centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    tempo = librosa.feature.tempo(y=y,sr=sr)\n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    chromagram = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "    return [mfcc,rolloff,centroid,rms,tempo,onset_env,zcr,chromagram,pitches]\n",
    "\n",
    "def extract_librosa_features(librosa_df):\n",
    "    \"\"\"\n",
    "    Extracts features and labels from librosa y and sr listed in the DataFrame and concatenates the\n",
    "    features with their labels into a matrix.\n",
    "\n",
    "    Parameters:\n",
    "        librosa_df (pandas.DataFrame): A DataFrame with y and sr values for audio clips with their genres\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A matrix of features along with labels.\n",
    "    \"\"\"\n",
    "    all_features = []  # List to store all extracted features\n",
    "    iters = 0\n",
    "    for index, row in librosa_df.iterrows():\n",
    "        print(iters)\n",
    "        iters += 1\n",
    "\n",
    "        obj_features = get_features(row['y'], row['sr'])\n",
    "        obj_features.append(row['genre'])\n",
    "        all_features.append(obj_features)\n",
    "        \n",
    "    # Return the numpy array of all extracted features along with corresponding genres\n",
    "    return np.array(all_features)\n",
    "\n",
    "lib_features = extract_librosa_features(matched_librosa_df)\n",
    "\n",
    "with lzma.open(\"labeled_lib_features.xz\", \"wb\") as f:\n",
    "    pickle.dump(lib_features, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition Dataset into Training, Validation, and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def balance_data(data_array, genres = genre_list):\n",
    "    # create a data frame with the last column titled genre\n",
    "    name_list = ['data'] * (data_array.shape[1] - 1)\n",
    "    name_list.append('genre')\n",
    "    data_df = pd.DataFrame(data_array)\n",
    "    data_df.columns = name_list\n",
    "    \n",
    "    genre_balance = np.zeros(len(genres))\n",
    "    for index, row in data_df.iterrows():\n",
    "        genre_balance[int(row['genre'])] += 1\n",
    "    max_samples = int(max(genre_balance))\n",
    "\n",
    "    # resample other genres to have that many samples\n",
    "    df_list = []\n",
    "    for genre_num in range(len(genres)):\n",
    "        df_list.append(data_df[data_df['genre'] == genre_num])\n",
    "\n",
    "    balanced_list = []\n",
    "    for df in df_list:\n",
    "        if (len(df) != max_samples):\n",
    "            df = resample(df, random_state=42, n_samples=max_samples, replace=True)\n",
    "        balanced_list.append(df)\n",
    "\n",
    "    balanced_df = pd.concat(balanced_list)\n",
    "    balanced_array = balanced_df.to_numpy()\n",
    "    \n",
    "    return balanced_array\n",
    "\n",
    "# Shuffle the features\n",
    "labeled_features = np.random.permutation(labeled_features)\n",
    "\n",
    "# Partition the Dataset into 3 Sets: Training, Validation, and Test\n",
    "num = len(labeled_features)\n",
    "# Calculate the number of samples for training data (60% of the dataset)\n",
    "num_training = int(num * 0.6)\n",
    "# Calculate the number of samples for validation data (20% of the dataset)\n",
    "num_validation = int(num * 0.8)\n",
    "\n",
    "# Extract the training data (60% of the labeled features)\n",
    "training_data = balance_data(labeled_features[:num_training])\n",
    "# Extract the validation data (20% of the labeled features)\n",
    "validation_data = balance_data(labeled_features[num_training:num_validation])\n",
    "# Extract the test data (remaining 20% of the labeled features)\n",
    "test_data = (labeled_features[num_validation:])\n",
    "\n",
    "# Separate the features from the labels\n",
    "num_cols = training_data.shape[1] - 1\n",
    "# Extract features from the training data\n",
    "training_features = training_data[:, :num_cols]\n",
    "# Extract features from the validation data\n",
    "validation_features = validation_data[:, :num_cols]\n",
    "# Extract features from the test data\n",
    "test_features = test_data[:, :num_cols]\n",
    "\n",
    "# Format the features for this multi-class classification problem\n",
    "num_classes = len(genre_list)\n",
    "# Extract years from the training data\n",
    "training_labels = training_data[:, num_cols].astype(int)\n",
    "# Extract years from the validation data\n",
    "validation_labels = validation_data[:, num_cols].astype(int)\n",
    "# Extract years from the test data\n",
    "test_labels = test_data[:, num_cols].astype(int)\n",
    "\n",
    "print(test_labels)  # Print the first 10 test labels\n",
    "print(to_categorical((test_labels)))  # Print the one-hot encoding of the first 10 test labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Neural Network (probably use pytorch instead of Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import metrics\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pretty_midi as pm\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "normalizer = keras.layers.BatchNormalization()\n",
    "model = keras.Sequential([\n",
    "    normalizer,\n",
    "    keras.layers.Dense(21, activation='relu'), # There are 20 variables in the feature matrix\n",
    "    keras.layers.Dense(15, activation='relu'), # 2/3 the input layer + 1\n",
    "    keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "\"\"\"\n",
    "optimizer=\"adam\": The optimizer algorithm to use during training. \n",
    "Adam optimizer is chosen, which is a popular optimization algorithm known for its efficiency.\n",
    "\n",
    "loss='categorical_crossentropy': The loss function used to measure the discrepancy between the \n",
    "predicted output and the true output labels. Categorical cross-entropy is suitable for\n",
    "multi-class classification tasks.\n",
    "\n",
    "metrics=['accuracy']: The metric(s) to be evaluated during training and testing. \n",
    "Accuracy is a commonly used metric to assess the model's performance.\n",
    "\"\"\"\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "training_features, train_labels: Input features and corresponding labels for model training.\n",
    "\n",
    "validation_features, val_labels: Validation set used to monitor the model's performance \n",
    "                                         during training.\n",
    "\n",
    "batch_size=32: Number of samples per gradient update. Training data is divided into batches, \n",
    "               and the model's weights are updated after each batch.\n",
    "\n",
    "epochs=50: Number of times the model will iterate over the entire training dataset.\n",
    "\n",
    "callbacks: EarlyStopping to stop training if the validation loss does not improve for a certain \n",
    "           number of epochs, and ModelCheckpoint to save the best model based on validation loss.\n",
    "\"\"\"\n",
    "\n",
    "# Encode the training and validation labels using one-hot encoding\n",
    "train_labels_encoded = to_categorical(training_labels)\n",
    "val_labels_encoded = to_categorical(validation_labels)\n",
    "\n",
    "history = model.fit(x=training_features, y=train_labels_encoded, \n",
    "                    validation_data=(validation_features, val_labels_encoded),\n",
    "                    batch_size=10, epochs=200, verbose=2,\n",
    "                    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n",
    "                               keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True)])\n",
    "\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.ylim([0, 10])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error [MPG]')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# Save the entire model to an h5 file\n",
    "model.save(\"my_model.h5\")\n",
    "\n",
    "# Use the loaded model for prediction\n",
    "real_labels = [0,0,0,0]\n",
    "predicted_labels = [0,0,0,0]\n",
    "correct = [0,0,0,0]\n",
    "\n",
    "preds = model.predict(test_features)\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    real = test_labels[i]\n",
    "    prediction = np.argmax(preds[i])\n",
    "    real_labels[real] += 1\n",
    "    predicted_labels[prediction] += 1\n",
    "\n",
    "    print(\"Label:      \" + str(real))\n",
    "    print(\"Prediction: \" + str(prediction))\n",
    "\n",
    "    if (real == prediction):\n",
    "        correct[real] += 1\n",
    "\n",
    "print(\"Total labels in test data: \", real_labels)\n",
    "print(\"Total labels in pred data: \", predicted_labels)\n",
    "print(\"correct labels in test data: \", correct)\n",
    "probabilities = [i / j for i, j in zip(correct, predicted_labels)]\n",
    "print(\"prediction probabilities: \", probabilities)\n",
    "    \n",
    "danube_midi = pm.PrettyMIDI('The-Blue-Danube-Waltz.mid')\n",
    "danube_feats = get_features(danube_midi)\n",
    "danube_feats = np.asarray(danube_feats)\n",
    "danube_feats = np.expand_dims(danube_feats, axis = 0)\n",
    "print('danube: ', np.argmax(model.predict(danube_feats)))\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_features, to_categorical(test_labels))\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
