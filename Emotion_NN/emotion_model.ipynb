{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Emotion Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Install Miniconda from their website\n",
    "* Put Conda on PATH during the install\n",
    "* Update conda - Terminal Command: conda update conda\n",
    "* Clone the Github repository (send Tim or I your github username if you havenâ€™t yet).\n",
    "* Load environment.yaml into conda environment (this will take a while to download) - Terminal Command: conda env create -n mus2vid -f environment.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Music and Emotion data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jamendo/DEAM dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process wav files and extract features\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# openSMILE features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opensmile\n",
      "  Downloading opensmile-2.4.2-py3-none-any.whl (4.5 MB)\n",
      "     ---------------------------------------- 4.5/4.5 MB 15.0 MB/s eta 0:00:00\n",
      "Collecting audobject>=0.6.1\n",
      "  Downloading audobject-0.7.9-py3-none-any.whl (24 kB)\n",
      "Collecting audinterface>=0.7.0\n",
      "  Downloading audinterface-1.0.3-py3-none-any.whl (31 kB)\n",
      "Collecting audformat<2.0.0,>=1.0.1\n",
      "  Downloading audformat-1.0.1-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.8/65.8 kB ? eta 0:00:00\n",
      "Collecting audmath>=1.2.1\n",
      "  Downloading audmath-1.2.1-py3-none-any.whl (10 kB)\n",
      "Collecting audresample<2.0.0,>=1.1.0\n",
      "  Downloading audresample-1.3.2-py3-none-win_amd64.whl (182 kB)\n",
      "     ------------------------------------- 182.6/182.6 kB 10.8 MB/s eta 0:00:00\n",
      "Collecting audeer>=1.18.0\n",
      "  Downloading audeer-1.20.1-py3-none-any.whl (23 kB)\n",
      "Collecting oyaml\n",
      "  Downloading oyaml-1.0-py2.py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages (from audobject>=0.6.1->opensmile) (6.6.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages (from audeer>=1.18.0->audobject>=0.6.1->opensmile) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in c:\\users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (6.0)\n",
      "Collecting audiofile>=0.4.0\n",
      "  Downloading audiofile-1.2.1-py3-none-any.whl (14 kB)\n",
      "Collecting iso-639\n",
      "  Downloading iso-639-0.4.5.tar.gz (167 kB)\n",
      "     -------------------------------------- 167.4/167.4 kB 9.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting iso3166\n",
      "  Downloading iso3166-2.1.1-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: pandas>=1.4.1 in c:\\users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages (from audmath>=1.2.1->audinterface>=0.7.0->opensmile) (1.24.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages (from importlib-metadata>=4.8.0->audobject>=0.6.1->opensmile) (3.15.0)\n",
      "Requirement already satisfied: soundfile in c:\\users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages (from audiofile>=0.4.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (0.12.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages (from pandas>=1.4.1->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages (from pandas>=1.4.1->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages (from pandas>=1.4.1->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.8.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages (from tqdm->audeer>=1.18.0->audobject>=0.6.1->opensmile) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.4.1->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages (from soundfile->audiofile>=0.4.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages (from cffi>=1.0->soundfile->audiofile>=0.4.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.21)\n",
      "Building wheels for collected packages: iso-639\n",
      "  Building wheel for iso-639 (setup.py): started\n",
      "  Building wheel for iso-639 (setup.py): finished with status 'done'\n",
      "  Created wheel for iso-639: filename=iso_639-0.4.5-py3-none-any.whl size=168863 sha256=de43f02da253f950b425f56bc66905934928cf4482d1c431de3c815f2eb9c96f\n",
      "  Stored in directory: c:\\users\\jkamp\\appdata\\local\\pip\\cache\\wheels\\d8\\78\\cc\\5478ca3b1c3f602eae6f8cdbd78f909c0a0bfa0bbcb5c7771f\n",
      "Successfully built iso-639\n",
      "Installing collected packages: iso-639, oyaml, iso3166, audresample, audmath, audeer, audobject, audiofile, audformat, audinterface, opensmile\n",
      "Successfully installed audeer-1.20.1 audformat-1.0.1 audinterface-1.0.3 audiofile-1.2.1 audmath-1.2.1 audobject-0.7.9 audresample-1.3.2 iso-639-0.4.5 iso3166-2.1.1 opensmile-2.4.2 oyaml-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install opensmile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "LibsndfileError",
     "evalue": "Error opening 'C:\\\\Users\\\\jkamp\\\\OneDrive\\\\Desktop\\\\Purdue Classes\\\\Mus2Vid\\\\Mus2Vid-code\\\\Emotion_NN\\\\audio.wav': System error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopensmile\u001b[39;00m\n\u001b[0;32m      3\u001b[0m smile \u001b[39m=\u001b[39m opensmile\u001b[39m.\u001b[39mSmile(\n\u001b[0;32m      4\u001b[0m     feature_set\u001b[39m=\u001b[39mopensmile\u001b[39m.\u001b[39mFeatureSet\u001b[39m.\u001b[39mComParE_2016,\n\u001b[0;32m      5\u001b[0m     feature_level\u001b[39m=\u001b[39mopensmile\u001b[39m.\u001b[39mFeatureLevel\u001b[39m.\u001b[39mFunctionals,\n\u001b[0;32m      6\u001b[0m )\n\u001b[1;32m----> 7\u001b[0m y \u001b[39m=\u001b[39m smile\u001b[39m.\u001b[39;49mprocess_file(\u001b[39m'\u001b[39;49m\u001b[39maudio.wav\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(y)\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(y))\n",
      "File \u001b[1;32mc:\\Users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages\\audinterface\\core\\feature.py:433\u001b[0m, in \u001b[0;36mFeature.process_file\u001b[1;34m(self, file, start, end, root)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_file\u001b[39m(\n\u001b[0;32m    407\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    408\u001b[0m         file: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    412\u001b[0m         root: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    413\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mDataFrame:\n\u001b[0;32m    414\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Extract features from an audio file.\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \n\u001b[0;32m    416\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    431\u001b[0m \n\u001b[0;32m    432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 433\u001b[0m     series \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess\u001b[39m.\u001b[39;49mprocess_file(\n\u001b[0;32m    434\u001b[0m         file,\n\u001b[0;32m    435\u001b[0m         start\u001b[39m=\u001b[39;49mstart,\n\u001b[0;32m    436\u001b[0m         end\u001b[39m=\u001b[39;49mend,\n\u001b[0;32m    437\u001b[0m         root\u001b[39m=\u001b[39;49mroot,\n\u001b[0;32m    438\u001b[0m     )\n\u001b[0;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_series_to_frame(series)\n",
      "File \u001b[1;32mc:\\Users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages\\audinterface\\core\\process.py:327\u001b[0m, in \u001b[0;36mProcess.process_file\u001b[1;34m(self, file, start, end, root)\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_index_wo_segment(index, root)\n\u001b[0;32m    325\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m     y, files, starts, ends \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_file(\n\u001b[0;32m    328\u001b[0m         file,\n\u001b[0;32m    329\u001b[0m         root\u001b[39m=\u001b[39;49mroot,\n\u001b[0;32m    330\u001b[0m         start\u001b[39m=\u001b[39;49mstart,\n\u001b[0;32m    331\u001b[0m         end\u001b[39m=\u001b[39;49mend,\n\u001b[0;32m    332\u001b[0m     )\n\u001b[0;32m    334\u001b[0m     index \u001b[39m=\u001b[39m audformat\u001b[39m.\u001b[39msegmented_index(files, starts, ends)\n\u001b[0;32m    336\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages\\audinterface\\core\\process.py:259\u001b[0m, in \u001b[0;36mProcess._process_file\u001b[1;34m(self, file, idx, root, start, end)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39mif\u001b[39;00m end \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     end \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mto_timedelta(end, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_rate)\n\u001b[1;32m--> 259\u001b[0m signal, sampling_rate \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mread_audio(\n\u001b[0;32m    260\u001b[0m     file,\n\u001b[0;32m    261\u001b[0m     start\u001b[39m=\u001b[39;49mstart,\n\u001b[0;32m    262\u001b[0m     end\u001b[39m=\u001b[39;49mend,\n\u001b[0;32m    263\u001b[0m     root\u001b[39m=\u001b[39;49mroot,\n\u001b[0;32m    264\u001b[0m )\n\u001b[0;32m    266\u001b[0m y, files, starts, ends \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_signal(\n\u001b[0;32m    267\u001b[0m     signal,\n\u001b[0;32m    268\u001b[0m     sampling_rate,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    271\u001b[0m     file\u001b[39m=\u001b[39mfile,\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    274\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwin_dur \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages\\audinterface\\core\\utils.py:150\u001b[0m, in \u001b[0;36mread_audio\u001b[1;34m(file, start, end, root)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     duration \u001b[39m=\u001b[39m end\u001b[39m.\u001b[39mtotal_seconds() \u001b[39m-\u001b[39m offset\n\u001b[1;32m--> 150\u001b[0m signal, sampling_rate \u001b[39m=\u001b[39m af\u001b[39m.\u001b[39;49mread(\n\u001b[0;32m    151\u001b[0m     audeer\u001b[39m.\u001b[39;49msafe_path(file),\n\u001b[0;32m    152\u001b[0m     always_2d\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    153\u001b[0m     offset\u001b[39m=\u001b[39;49moffset,\n\u001b[0;32m    154\u001b[0m     duration\u001b[39m=\u001b[39;49mduration,\n\u001b[0;32m    155\u001b[0m )\n\u001b[0;32m    157\u001b[0m \u001b[39mreturn\u001b[39;00m signal, sampling_rate\n",
      "File \u001b[1;32mc:\\Users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages\\audiofile\\core\\io.py:411\u001b[0m, in \u001b[0;36mread\u001b[1;34m(file, duration, offset, always_2d, dtype, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[39mif\u001b[39;00m duration \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m duration \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    408\u001b[0m         duration \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\n\u001b[0;32m    409\u001b[0m             np\u001b[39m.\u001b[39mceil(duration \u001b[39m*\u001b[39m sampling_rate) \u001b[39m+\u001b[39m offset\n\u001b[0;32m    410\u001b[0m         )  \u001b[39m# samples\u001b[39;00m\n\u001b[1;32m--> 411\u001b[0m     signal, sampling_rate \u001b[39m=\u001b[39m soundfile\u001b[39m.\u001b[39mread(\n\u001b[0;32m    412\u001b[0m         file,\n\u001b[0;32m    413\u001b[0m         start\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(offset),\n\u001b[0;32m    414\u001b[0m         stop\u001b[39m=\u001b[39mduration,\n\u001b[0;32m    415\u001b[0m         dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m    416\u001b[0m         always_2d\u001b[39m=\u001b[39malways_2d,\n\u001b[0;32m    417\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    418\u001b[0m     )\n\u001b[0;32m    419\u001b[0m \u001b[39m# [samples, channels] => [channels, samples]\u001b[39;00m\n\u001b[0;32m    420\u001b[0m signal \u001b[39m=\u001b[39m signal\u001b[39m.\u001b[39mT\n",
      "File \u001b[1;32mc:\\Users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages\\soundfile.py:285\u001b[0m, in \u001b[0;36mread\u001b[1;34m(file, frames, start, stop, dtype, always_2d, fill_value, out, samplerate, channels, format, subtype, endian, closefd)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread\u001b[39m(file, frames\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, start\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, stop\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfloat64\u001b[39m\u001b[39m'\u001b[39m, always_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    200\u001b[0m          fill_value\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, samplerate\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, channels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    201\u001b[0m          \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, subtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, endian\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, closefd\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    202\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Provide audio data from a sound file as NumPy array.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \n\u001b[0;32m    204\u001b[0m \u001b[39m    By default, the whole file is read from the beginning, but the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    283\u001b[0m \n\u001b[0;32m    284\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 285\u001b[0m     \u001b[39mwith\u001b[39;00m SoundFile(file, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m, samplerate, channels,\n\u001b[0;32m    286\u001b[0m                    subtype, endian, \u001b[39mformat\u001b[39;49m, closefd) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    287\u001b[0m         frames \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39m_prepare_read(start, stop, frames)\n\u001b[0;32m    288\u001b[0m         data \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread(frames, dtype, always_2d, fill_value, out)\n",
      "File \u001b[1;32mc:\\Users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages\\soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode \u001b[39m=\u001b[39m mode\n\u001b[0;32m    656\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info \u001b[39m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    657\u001b[0m                                  \u001b[39mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 658\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(file, mode_int, closefd)\n\u001b[0;32m    659\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mset\u001b[39m(mode)\u001b[39m.\u001b[39missuperset(\u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseekable():\n\u001b[0;32m    660\u001b[0m     \u001b[39m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[0;32m    661\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jkamp\\miniconda3\\envs\\mus2vid\\lib\\site-packages\\soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m file_ptr \u001b[39m==\u001b[39m _ffi\u001b[39m.\u001b[39mNULL:\n\u001b[0;32m   1214\u001b[0m     \u001b[39m# get the actual error code\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m     err \u001b[39m=\u001b[39m _snd\u001b[39m.\u001b[39msf_error(file_ptr)\n\u001b[1;32m-> 1216\u001b[0m     \u001b[39mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError opening \u001b[39m\u001b[39m{0!r}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname))\n\u001b[0;32m   1217\u001b[0m \u001b[39mif\u001b[39;00m mode_int \u001b[39m==\u001b[39m _snd\u001b[39m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1218\u001b[0m     \u001b[39m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m     \u001b[39m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m     \u001b[39m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n\u001b[0;32m   1221\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mframes \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mLibsndfileError\u001b[0m: Error opening 'C:\\\\Users\\\\jkamp\\\\OneDrive\\\\Desktop\\\\Purdue Classes\\\\Mus2Vid\\\\Mus2Vid-code\\\\Emotion_NN\\\\audio.wav': System error."
     ]
    }
   ],
   "source": [
    "import opensmile\n",
    "\n",
    "def get_matched_smile(audio, genres):\n",
    "    smile = opensmile.Smile(\n",
    "        feature_set=opensmile.FeatureSet.emobase,\n",
    "        feature_level=opensmile.FeatureLevel.Functionals,\n",
    "    )\n",
    "\n",
    "    all_smiles = [] # list of smile features for each clip\n",
    "    iters = 0\n",
    "    for file in audio:\n",
    "        iters += 1\n",
    "        print(str(iters) + \"/\" + str(len(audio)))\n",
    "        \n",
    "        smile_feats = smile.process_file('wav_clips/' + file)\n",
    "        all_smiles.append(smile_feats)\n",
    "\n",
    "    df = pd.DataFrame({'features': all_smiles, 'genre': genres})\n",
    "    \n",
    "    return df\n",
    "\n",
    "matched_smile_df = get_matched_smile(audio_clips, clip_genres)\n",
    "\n",
    "# Optionally save the matched_midi_df DataFrame as a pickle file    \n",
    "with lzma.open(\"matched_smile.xz\", \"wb\") as f:\n",
    "    pickle.dump(matched_smile_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider using MIDI data for this as well as librosa, or just librosa features that Palm found in a paper\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pickle\n",
    "import lzma\n",
    "\n",
    "\n",
    "def get_matched_librosa(audio, genres):\n",
    "    \"\"\"\n",
    "    Takes in a list of audio filepaths and creates a dataframe column of mel spectrograms with matched\n",
    "    genres, similar to the midi object dataframe\n",
    "    \"\"\"\n",
    "    ys = []\n",
    "    srs = []\n",
    "\n",
    "    iters = 0\n",
    "    for file in audio:\n",
    "        print(str(iters) + \"/\" + str(len(audio)))\n",
    "        iters += 1\n",
    "\n",
    "        y, sr = librosa.load(\"wav_clips/\" + file)\n",
    "        ys.append(y)\n",
    "        srs.append(srs)\n",
    "\n",
    "    df = pd.DataFrame({'y': ys, 'sr': srs, 'genre': genres})\n",
    "    \n",
    "    return df\n",
    "\n",
    "matched_librosa_df = get_matched_librosa(audio_clips, clip_genres)\n",
    "\n",
    "\n",
    "\n",
    "with lzma.open(\"matched_librosa.xz\", \"wb\") as f:\n",
    "    pickle.dump(matched_librosa_df, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pickle\n",
    "import lzma\n",
    "\n",
    "def get_features(y, sr):\n",
    "    mfcc = librosa.feature.mfcc(y=y,sr=sr)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y,sr=sr)\n",
    "    centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    tempo = librosa.feature.tempo(y=y,sr=sr)\n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    chromagram = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "    return [mfcc,rolloff,centroid,rms,tempo,onset_env,zcr,chromagram,pitches]\n",
    "\n",
    "def extract_librosa_features(librosa_df):\n",
    "    \"\"\"\n",
    "    Extracts features and labels from librosa y and sr listed in the DataFrame and concatenates the\n",
    "    features with their labels into a matrix.\n",
    "\n",
    "    Parameters:\n",
    "        librosa_df (pandas.DataFrame): A DataFrame with y and sr values for audio clips with their genres\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A matrix of features along with labels.\n",
    "    \"\"\"\n",
    "    all_features = []  # List to store all extracted features\n",
    "    iters = 0\n",
    "    for index, row in librosa_df.iterrows():\n",
    "        print(iters)\n",
    "        iters += 1\n",
    "\n",
    "        obj_features = get_features(row['y'], row['sr'])\n",
    "        obj_features.append(row['genre'])\n",
    "        all_features.append(obj_features)\n",
    "        \n",
    "    # Return the numpy array of all extracted features along with corresponding genres\n",
    "    return np.array(all_features)\n",
    "\n",
    "lib_features = extract_librosa_features(matched_librosa_df)\n",
    "\n",
    "with lzma.open(\"labeled_lib_features.xz\", \"wb\") as f:\n",
    "    pickle.dump(lib_features, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition Dataset into Training, Validation, and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def balance_data(data_array, genres = genre_list):\n",
    "    # create a data frame with the last column titled genre\n",
    "    name_list = ['data'] * (data_array.shape[1] - 1)\n",
    "    name_list.append('genre')\n",
    "    data_df = pd.DataFrame(data_array)\n",
    "    data_df.columns = name_list\n",
    "    \n",
    "    genre_balance = np.zeros(len(genres))\n",
    "    for index, row in data_df.iterrows():\n",
    "        genre_balance[int(row['genre'])] += 1\n",
    "    max_samples = int(max(genre_balance))\n",
    "\n",
    "    # resample other genres to have that many samples\n",
    "    df_list = []\n",
    "    for genre_num in range(len(genres)):\n",
    "        df_list.append(data_df[data_df['genre'] == genre_num])\n",
    "\n",
    "    balanced_list = []\n",
    "    for df in df_list:\n",
    "        if (len(df) != max_samples):\n",
    "            df = resample(df, random_state=42, n_samples=max_samples, replace=True)\n",
    "        balanced_list.append(df)\n",
    "\n",
    "    balanced_df = pd.concat(balanced_list)\n",
    "    balanced_array = balanced_df.to_numpy()\n",
    "    \n",
    "    return balanced_array\n",
    "\n",
    "# Shuffle the features\n",
    "labeled_features = np.random.permutation(labeled_features)\n",
    "\n",
    "# Partition the Dataset into 3 Sets: Training, Validation, and Test\n",
    "num = len(labeled_features)\n",
    "# Calculate the number of samples for training data (60% of the dataset)\n",
    "num_training = int(num * 0.6)\n",
    "# Calculate the number of samples for validation data (20% of the dataset)\n",
    "num_validation = int(num * 0.8)\n",
    "\n",
    "# Extract the training data (60% of the labeled features)\n",
    "training_data = balance_data(labeled_features[:num_training])\n",
    "# Extract the validation data (20% of the labeled features)\n",
    "validation_data = balance_data(labeled_features[num_training:num_validation])\n",
    "# Extract the test data (remaining 20% of the labeled features)\n",
    "test_data = (labeled_features[num_validation:])\n",
    "\n",
    "# Separate the features from the labels\n",
    "num_cols = training_data.shape[1] - 1\n",
    "# Extract features from the training data\n",
    "training_features = training_data[:, :num_cols]\n",
    "# Extract features from the validation data\n",
    "validation_features = validation_data[:, :num_cols]\n",
    "# Extract features from the test data\n",
    "test_features = test_data[:, :num_cols]\n",
    "\n",
    "# Format the features for this multi-class classification problem\n",
    "num_classes = len(genre_list)\n",
    "# Extract years from the training data\n",
    "training_labels = training_data[:, num_cols].astype(int)\n",
    "# Extract years from the validation data\n",
    "validation_labels = validation_data[:, num_cols].astype(int)\n",
    "# Extract years from the test data\n",
    "test_labels = test_data[:, num_cols].astype(int)\n",
    "\n",
    "print(test_labels)  # Print the first 10 test labels\n",
    "print(to_categorical((test_labels)))  # Print the one-hot encoding of the first 10 test labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Neural Network (probably use pytorch instead of Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import metrics\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pretty_midi as pm\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "normalizer = keras.layers.BatchNormalization()\n",
    "model = keras.Sequential([\n",
    "    normalizer,\n",
    "    keras.layers.Dense(21, activation='relu'), # There are 20 variables in the feature matrix\n",
    "    keras.layers.Dense(15, activation='relu'), # 2/3 the input layer + 1\n",
    "    keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "\"\"\"\n",
    "optimizer=\"adam\": The optimizer algorithm to use during training. \n",
    "Adam optimizer is chosen, which is a popular optimization algorithm known for its efficiency.\n",
    "\n",
    "loss='categorical_crossentropy': The loss function used to measure the discrepancy between the \n",
    "predicted output and the true output labels. Categorical cross-entropy is suitable for\n",
    "multi-class classification tasks.\n",
    "\n",
    "metrics=['accuracy']: The metric(s) to be evaluated during training and testing. \n",
    "Accuracy is a commonly used metric to assess the model's performance.\n",
    "\"\"\"\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "training_features, train_labels: Input features and corresponding labels for model training.\n",
    "\n",
    "validation_features, val_labels: Validation set used to monitor the model's performance \n",
    "                                         during training.\n",
    "\n",
    "batch_size=32: Number of samples per gradient update. Training data is divided into batches, \n",
    "               and the model's weights are updated after each batch.\n",
    "\n",
    "epochs=50: Number of times the model will iterate over the entire training dataset.\n",
    "\n",
    "callbacks: EarlyStopping to stop training if the validation loss does not improve for a certain \n",
    "           number of epochs, and ModelCheckpoint to save the best model based on validation loss.\n",
    "\"\"\"\n",
    "\n",
    "# Encode the training and validation labels using one-hot encoding\n",
    "train_labels_encoded = to_categorical(training_labels)\n",
    "val_labels_encoded = to_categorical(validation_labels)\n",
    "\n",
    "history = model.fit(x=training_features, y=train_labels_encoded, \n",
    "                    validation_data=(validation_features, val_labels_encoded),\n",
    "                    batch_size=10, epochs=200, verbose=2,\n",
    "                    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n",
    "                               keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True)])\n",
    "\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.ylim([0, 10])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error [MPG]')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# Save the entire model to an h5 file\n",
    "model.save(\"my_model.h5\")\n",
    "\n",
    "# Use the loaded model for prediction\n",
    "real_labels = [0,0,0,0]\n",
    "predicted_labels = [0,0,0,0]\n",
    "correct = [0,0,0,0]\n",
    "\n",
    "preds = model.predict(test_features)\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    real = test_labels[i]\n",
    "    prediction = np.argmax(preds[i])\n",
    "    real_labels[real] += 1\n",
    "    predicted_labels[prediction] += 1\n",
    "\n",
    "    print(\"Label:      \" + str(real))\n",
    "    print(\"Prediction: \" + str(prediction))\n",
    "\n",
    "    if (real == prediction):\n",
    "        correct[real] += 1\n",
    "\n",
    "print(\"Total labels in test data: \", real_labels)\n",
    "print(\"Total labels in pred data: \", predicted_labels)\n",
    "print(\"correct labels in test data: \", correct)\n",
    "probabilities = [i / j for i, j in zip(correct, predicted_labels)]\n",
    "print(\"prediction probabilities: \", probabilities)\n",
    "    \n",
    "danube_midi = pm.PrettyMIDI('The-Blue-Danube-Waltz.mid')\n",
    "danube_feats = get_features(danube_midi)\n",
    "danube_feats = np.asarray(danube_feats)\n",
    "danube_feats = np.expand_dims(danube_feats, axis = 0)\n",
    "print('danube: ', np.argmax(model.predict(danube_feats)))\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_features, to_categorical(test_labels))\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mus2vid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
