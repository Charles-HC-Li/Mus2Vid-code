{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wreuzP62Wc11"
   },
   "source": [
    "# Music Genre Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jsm0S0H5WiGN",
    "outputId": "b98d9b52-b483-4ce0-b5fa-1395e9fc880b"
   },
   "outputs": [],
   "source": [
    "!pip install pretty_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KZx1BNiqWk3N",
    "outputId": "10853909-750f-47b8-c339-3376baeb817e"
   },
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWLlSBsi8uBw"
   },
   "outputs": [],
   "source": [
    "# Credit: Code adapted and used from Sander Shi's Colab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ksk8OirqWc14"
   },
   "source": [
    "## Download and Parse Genre Labels\n",
    "* Go to the website http://www.tagtraum.com/msd_genre_datasets.html.\n",
    "* Look for the section labeled \"CD1\" and download the associated zip file.\n",
    "* Once the download is complete, unzip the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TEyTZ3teXmN8",
    "outputId": "0eda93ba-6c97-460e-e010-4325c1b70407"
   },
   "outputs": [],
   "source": [
    "# Unzips the \"CD1\" zip file\n",
    "!unzip msd_tagtraum_cd1.cls.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mI4-EH5IWc15",
    "outputId": "dd21f9ea-a897-4695-9b97-4aed650d3109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Genre             TrackID\n",
      "0  Pop_Rock  TRAAAAK128F9318786\n",
      "1       Rap  TRAAAAW128F429D538\n",
      "2  Pop_Rock  TRAAABD128F429CF47\n",
      "3      Jazz  TRAAAED128E0783FAB\n",
      "4  Pop_Rock  TRAAAEF128F4273421\n",
      "\n",
      "['Reggae', 'Country', 'Rap', 'New Age', 'RnB', 'Jazz', 'International', 'Electronic', 'Blues', 'Folk', 'Pop_Rock', 'Vocal', 'Latin']\n",
      "\n",
      "{'Reggae': 0, 'Country': 1, 'Rap': 2, 'New Age': 3, 'RnB': 4, 'Jazz': 5, 'International': 6, 'Electronic': 7, 'Blues': 8, 'Folk': 9, 'Pop_Rock': 10, 'Vocal': 11, 'Latin': 12}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_genres(path):\n",
    "    \"\"\"\n",
    "    Stores the genre labels into a pandas data frame.\n",
    "    \n",
    "    Parameters:\n",
    "        path (str): The path to the genre label file.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A data frame containing the genres and MIDI IDs.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    genres = []\n",
    "    \n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            # Skip lines starting with '#'\n",
    "            if not line.startswith('#'):\n",
    "                # Splits the line by the tab character ('\\t') and unpacks the resulting values \n",
    "                # into variables x and y. The strip() function removes leading and trailing whitespace from the line.\n",
    "                x, y, *_ = line.strip().split(\"\\t\")\n",
    "                # Appends the value of x (track ID) to the ids list.\n",
    "                ids.append(x)\n",
    "                # Appends the value of y (genre) to the genres list.\n",
    "                genres.append(y)\n",
    "    \n",
    "    # Constructs a data frame with two columns, \"Genre\" and \"TrackID\", using a dictionary. \n",
    "    # The \"Genre\" column contains the genres stored in the genres list, and the \"TrackID\" column \n",
    "    # contains the track IDs stored in the ids list.\n",
    "    genre_df = pd.DataFrame(data={\"Genre\": genres, \"TrackID\": ids})\n",
    "\n",
    "    return genre_df\n",
    "\n",
    "# genre_path: path of the unzipped \"CD1\" file\n",
    "genre_path = \"msd_tagtraum_cd1.cls\"\n",
    "# creates the genres data frame\n",
    "genre_df = get_genres(genre_path)\n",
    "\n",
    "# Get unique genre labels\n",
    "label_list = list(set(genre_df.Genre))\n",
    "\n",
    "# Create a dictionary mapping genre labels to their index\n",
    "label_dict = {lbl: label_list.index(lbl) for lbl in label_list}\n",
    "\n",
    "print(genre_df.head(), end=\"\\n\\n\")  # Display the first few rows of the genre data frame\n",
    "print(label_list, end=\"\\n\\n\")  # Display the unique genre labels\n",
    "print(label_dict, end=\"\\n\\n\")  # Display the genre dictionary mapping labels to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5SiYwTbgP59R",
    "outputId": "f39622b5-e65f-4c29-f170-41d12bd52445"
   },
   "outputs": [],
   "source": [
    "#cd /content/drive/MyDrive"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download CSV file with Classical Dataset and Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelin's code will create a list of .wav filepaths and years of composition\n",
    "# temporarily name these wavs and years\n",
    "import pretty_midi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from basic_pitch.inference import predict as bp_predict\n",
    "from basic_pitch import ICASSP_2022_MODEL_PATH\n",
    "BASIC_PITCH_MODEL = tf.saved_model.load(str(ICASSP_2022_MODEL_PATH))\n",
    "\n",
    "# Parameters\n",
    "STD_ONSET = 0.3\n",
    "STD_FRAME = 0.2\n",
    "STD_MIN_NOTE_LEN = 50\n",
    "STD_MIN_FREQ = None\n",
    "STD_MAX_FREQ = 3000\n",
    "\n",
    "def predict(filestr):\n",
    "    \"\"\"\n",
    "    Makes a Basic Pitch prediction with the global parameters above given an input audio file.\n",
    "    \n",
    "    Parameters:\n",
    "        filestr (str): The path to the input audio file.\n",
    "        \n",
    "    Returns:\n",
    "        PrettyMIDI object containing predicted MIDI notes.\n",
    "    \"\"\"\n",
    "    # Run prediction\n",
    "    model_output, midi_data, note_events = bp_predict(\n",
    "        filestr,\n",
    "        BASIC_PITCH_MODEL,\n",
    "        STD_ONSET,\n",
    "        STD_FRAME,\n",
    "        STD_MIN_NOTE_LEN,\n",
    "        STD_MIN_FREQ,\n",
    "        STD_MAX_FREQ\n",
    "    ) # midi_data is the PrettyMIDI object corresponding to the prediction\n",
    "    return midi_data\n",
    "\n",
    "wavs = []\n",
    "years = []\n",
    "prettys = []\n",
    "\n",
    "for filepath in wavs:\n",
    "    midi_obj = predict(filepath)\n",
    "    prettys.append(midi_obj)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1HeXBAkEWc15"
   },
   "source": [
    "## Download, Parse and Match Midi Files\n",
    "* Visit the website http://colinraffel.com/projects/lmd/.\n",
    "* Find the section titled \"LMD-matched\" and click on the provided link. This will initiate the download of a MIDI dataset where each file is matched to an entry in the million song dataset.\n",
    "* Once the download is complete, untar the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ba5x_vt4YX78",
    "outputId": "cc138f52-b57d-4779-f026-5a4260e9496c"
   },
   "outputs": [],
   "source": [
    "# Untars the downloaded file\n",
    "!tar -xvf lmd_matched.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r-0CxSkKWc16",
    "outputId": "93325826-9bee-45cc-e388-82989116c127"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Path, Genre]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def get_matched_midi(midi_folder, genre_df):\n",
    "    \"\"\"\n",
    "    Loads the MIDI file paths from the given folder and creates a pandas DataFrame.\n",
    "    Matches each MIDI file with a genre based on the genre_df generated by get_genres.\n",
    "\n",
    "    Parameters:\n",
    "        midi_folder (str): The path to the MIDI files folder.\n",
    "        genre_df (pandas.DataFrame): The genre label DataFrame generated by get_genres.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the track ID and the path to the corresponding MIDI file.\n",
    "    \"\"\"\n",
    "    # Get all MIDI files\n",
    "    track_ids = []\n",
    "    file_paths = []\n",
    "\n",
    "    for dir_name, subdir_list, file_list in os.walk(midi_folder):\n",
    "        # Checks if the length of the directory name is equal to 36. \n",
    "        if len(dir_name) == 36:\n",
    "            # Extracts the track ID from the directory name by slicing the string from index 18 to the end\n",
    "            track_id = dir_name[18:]\n",
    "            # Constructs the file path by joining the directory name and the first file in the file_list\n",
    "            # ***(assumes that each directory contains only one MIDI file)*** \n",
    "            # ATTENTION: some directories actually contain more than one MIDI file for that genre\n",
    "            # file_list[0] is done for Simplification sake\n",
    "            file_path = os.path.join(dir_name, file_list[0])\n",
    "            track_ids.append(track_id)\n",
    "            file_paths.append(file_path)\n",
    "\n",
    "    # Constructs a DataFrame with two columns, \"TrackID\" and \"Path\", using a dictionary. \n",
    "    # The \"TrackID\" column contains the track IDs stored in the track_ids list, and \n",
    "    # The \"Path\" column contains the file paths stored in the file_paths list\n",
    "    all_midi_df = pd.DataFrame({\"TrackID\": track_ids, \"Path\": file_paths})\n",
    "\n",
    "    # Inner join the frames with the genre DataFrame\n",
    "    df = pd.merge(all_midi_df, genre_df, on='TrackID', how='inner')\n",
    "    \n",
    "    # Drop the redundant TrackID column\n",
    "    df = df.drop([\"TrackID\"], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# midi_path: path to lmd_matched folder created from running previous tar command\n",
    "midi_path = \"lmd_matched\"\n",
    "# matched_midi_df: data frame with matched genres to file paths\n",
    "matched_midi_df = get_matched_midi(midi_path, genre_df)\n",
    "\n",
    "# Optionally save the matched_midi_df DataFrame as a pickle file\n",
    "with open(\"matched_midi.pkl\", \"wb\") as f:\n",
    "    pickle.dump(matched_midi_df, f)\n",
    "\n",
    "# Print the first few rows\n",
    "print(matched_midi_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OASzJAAaPxb4",
    "outputId": "dbdb510c-4431-41bf-e5dc-683c72a145c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7rEgO2gY9poS",
    "outputId": "e19acb69-5bfe-4379-ac2d-b15f61da3bdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Path     Genre\n",
      "0  lmd_matched\\A\\A\\A\\TRAAAGR128F425B14B\\1d9d16a9d...  Pop_Rock\n",
      "1  lmd_matched\\A\\A\\D\\TRAADKW128E079503A\\3797e9b9a...  Pop_Rock\n",
      "2  lmd_matched\\A\\A\\F\\TRAAFMT128F429DB58\\0a4f2051b...  Pop_Rock\n",
      "3  lmd_matched\\A\\A\\G\\TRAAGMC128F4292D0F\\0644195d1...   Country\n",
      "4  lmd_matched\\A\\A\\L\\TRAALAH128E078234A\\8cfecf566...  Pop_Rock\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# If saved, load matched_midi_df from the saved pickle file\n",
    "with open('matched_midi.pkl', 'rb') as f:\n",
    "    matched_midi_df = pickle.load(f)\n",
    "\n",
    "print(matched_midi_df.head())\n",
    "# print(len(matched_midi_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "a6F3c1s0Wc16"
   },
   "source": [
    "## Extract Midi Files Features\n",
    "\n",
    "* Ensure you have the necessary libraries installed, such as pretty_midi, numpy, librosa, fluidsynth, and pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8pSVM3XRbn4",
    "outputId": "01db45ed-6c33-4037-bcc0-c5fe3dc4e313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyfluidsynth in c:\\users\\jkamp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\jkamp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pyfluidsynth) (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "# Library needed for feature extraction\n",
    "!pip install pyfluidsynth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "pqLVfz5oWc16",
    "outputId": "aa64b17b-ffa6-4d3d-a96e-5a18536f7068",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lmd_matched\\A\\F\\Q\\TRAFQFM128E078EC97\\3d5a78da77f009eb88641731e91d6982.mid\n",
      "lmd_matched\\A\\K\\Y\\TRAKYHY128F933BB51\\818034a239a474977975b5d18a7ae15a.mid\n",
      "lmd_matched\\A\\Z\\C\\TRAZCCG128F1463460\\986d9f7a327bf3487c98c55bcf23c920.mid\n",
      "lmd_matched\\B\\E\\R\\TRBERAT128F428036D\\21a3deefcd110324ac64c96f3ad39eb3.mid\n",
      "lmd_matched\\B\\K\\F\\TRBKFKL128E078ED76\\50eaa6135fdff0b9aac62318330500f4.mid\n",
      "lmd_matched\\C\\A\\H\\TRCAHUS128F14648B1\\b7ce9466c5cecbf1f19520cbe0e3c8cc.mid\n",
      "lmd_matched\\C\\G\\V\\TRCGVHG128F42A6C1F\\d2a37714c56480ba377bd4eea234ff98.mid\n",
      "lmd_matched\\C\\L\\O\\TRCLOST128E079221A\\7c6c1eac12c9bc2bda71d4f98b9ee13c.mid\n",
      "lmd_matched\\D\\R\\F\\TRDRFEV128F429E604\\d89d5403ee99245aa11283a8cec10e8c.mid\n",
      "lmd_matched\\D\\W\\M\\TRDWMEN128F935A08C\\02b3449d6e4c6e29571b2272e11ab1b8.mid\n",
      "lmd_matched\\F\\A\\E\\TRFAEIY128F42619B5\\aef10f61a1505fa8333cddd166396aa6.mid\n",
      "lmd_matched\\F\\U\\S\\TRFUSOG128E078EC6F\\28cc1b9acc9f23505d1b97f969d6df5e.mid\n",
      "lmd_matched\\F\\Y\\V\\TRFYVIE128F148C86E\\aab67d778d956bee5d7135bbb0bba121.mid\n",
      "lmd_matched\\G\\D\\X\\TRGDXSF128F428B57C\\40140e92ac87961dff491b962cd16317.mid\n",
      "lmd_matched\\G\\H\\Q\\TRGHQUR128E0783EEB\\ac4c4c7da2de1a739a6b462c3eea110e.mid\n",
      "lmd_matched\\G\\O\\C\\TRGOCCP128F92F3AE3\\12990033cb9629058248356824976899.mid\n",
      "lmd_matched\\G\\S\\S\\TRGSSCI128F933D9C1\\49a4d9293cd53d082a50c24a94084e97.mid\n",
      "lmd_matched\\G\\V\\I\\TRGVIDQ128F92C79D8\\def082823a768154a9b50adbbea3bcaa.mid\n",
      "lmd_matched\\G\\W\\I\\TRGWIPY12903D00F3E\\1fea22683b6fe65756da4388816b7785.mid\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import librosa\n",
    "# import fluidsynth\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "import warnings\n",
    "from sklearn.utils import resample\n",
    "  \n",
    "\n",
    "def normalize_features(features):\n",
    "    \"\"\"\n",
    "    Normalizes the features to the range [-1, 1].\n",
    "\n",
    "    Parameters:\n",
    "        features (list of float): The array of features.\n",
    "\n",
    "    Returns:\n",
    "        list of float: Normalized features.\n",
    "    \"\"\"\n",
    "    # Normalize each feature based on its specific range\n",
    "    tempo = (features[0] - 150) / 300\n",
    "    num_sig_changes = (features[1] - 2) / 10\n",
    "    resolution = (features[2] - 260) / 400\n",
    "    time_sig_1 = (features[3] - 3) / 8\n",
    "    time_sig_2 = (features[4] - 3) / 8\n",
    "    melody_complexity = (features[5] - 0) / 10\n",
    "    melody_range = (features[6] - 0) / 80\n",
    "\n",
    "    # Normalize pitch class histogram\n",
    "    pitch_class_hist = [((f - 0) / 100) for f in features[7:-1]]\n",
    "\n",
    "    # Return the normalized feature vector\n",
    "    return [tempo, resolution, time_sig_1, time_sig_2, melody_complexity, melody_range] + pitch_class_hist\n",
    "\n",
    "\n",
    "\n",
    "def get_features(path):\n",
    "    \"\"\"\n",
    "    Extracts specific features from a MIDI file given its path using the pretty_midi library.\n",
    "    Handle any potential errors with MIDI files appropriately.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): The path to the MIDI file.\n",
    "\n",
    "    Returns:\n",
    "        list of float: The extracted features.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Checking for MIDI files\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"error\")\n",
    "\n",
    "            # Creates a PrettyMIDI object by loading the MIDI file specified by the given path.\n",
    "            file = pretty_midi.PrettyMIDI(path)\n",
    "            \n",
    "            # tempo: the estimated tempo of the audio file\n",
    "            tempo = file.estimate_tempo()\n",
    "\n",
    "            # num_sig_changes: the number of time signature changes in the audio file\n",
    "            num_sig_changes = len(file.time_signature_changes)\n",
    "\n",
    "            # resolution: the time resolution of the audio file (in ticks per beat)\n",
    "            resolution = file.resolution\n",
    "\n",
    "\n",
    "            # Extract time signature information\n",
    "            ts_changes = file.time_signature_changes\n",
    "            ts_1, ts_2 = 4, 4\n",
    "            if len(ts_changes) > 0:\n",
    "                ts_1 = ts_changes[0].numerator\n",
    "                ts_2 = ts_changes[0].denominator\n",
    "            \n",
    "            # Extract melody-related features\n",
    "            # melody: a pitch class histogram of the audio file\n",
    "            melody = file.get_pitch_class_histogram()\n",
    "            # melody_complexity: the number of unique pitch classes in the melody\n",
    "            melody_complexity = np.sum(melody > 0)\n",
    "            # melody_range: the range of pitch classes in the melody\n",
    "            melody_range = np.max(melody) - np.min(melody)\n",
    "            # OPTIONAL feature melody_contour: the temporal evolution of pitch content in the audio file\n",
    "            # melody_contour = librosa.feature.tempogram(y=file.fluidsynth(fs=16000), sr=16000, hop_length=512)\n",
    "            # melody_contour = np.mean(melody_contour, axis=0)\n",
    "            # chroma: a chroma representation of the audio file\n",
    "            chroma = file.get_chroma()\n",
    "            # pitch_class_hist: the sum of the chroma matrix along the pitch axis\n",
    "            pitch_class_hist = np.sum(chroma, axis=1)\n",
    "\n",
    "            return normalize_features([tempo, num_sig_changes, resolution, ts_1,\n",
    "                            ts_2, melody_complexity, melody_range] + list(pitch_class_hist)) # + list(melody_contour))\n",
    "            \n",
    "    # Discard MIDI file if there is an error\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "            \n",
    "\n",
    "def extract_midi_features(path_df, oversample=True, undersample=False):\n",
    "    \"\"\"\n",
    "    Extracts features and labels from MIDI files listed in the path DataFrame and concatenates the\n",
    "    features with their labels into a matrix.\n",
    "\n",
    "    Since the dataset is inherently unbalanced in terms of genre distribution, oversampling and \n",
    "    undersampling can be used to achieve a more balanced representation of features for each genre.\n",
    "\n",
    "    Parameters:\n",
    "        path_df (pandas.DataFrame): A DataFrame with paths to MIDI files and their matched genre.\n",
    "        oversample (bool): Whether or not to perform oversampling on the data. Defaults to False.\n",
    "        undersample (bool): Whether or not to perform undersampling on the data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A matrix of features along with labels.\n",
    "    \"\"\"\n",
    "    all_features = []  # List to store all extracted features\n",
    "    max_count = 0  # Variable to track the maximum count of MIDI files in a genre\n",
    "\n",
    "    # Iterate through each genre in label_dict\n",
    "    for genre in label_dict.keys(): #  Genre is the string from label_dict\n",
    "        genre_df = path_df.loc[path_df['Genre'] == genre]  # DataFrame containing MIDI files of the current genre\n",
    "        genre_count = len(genre_df)  # Count of MIDI files in the current genre\n",
    "        if genre_count > max_count:\n",
    "            max_count = genre_count  # Update the maximum count if the current genre has more MIDI files\n",
    "\n",
    "        features_list = []  # List to store features of MIDI files in the current genre\n",
    "        for index, row in genre_df.iterrows():\n",
    "            # Extract features from MIDI file\n",
    "            features = get_features(row.Path)\n",
    "            # Map genre label to a number\n",
    "            genre = label_dict[row.Genre]\n",
    "            if features is not None:\n",
    "                # Append the genre label to the feature vector\n",
    "                # (i.e., it concatenates the features with the labels into a matrix)\n",
    "                features.append(genre)\n",
    "                features_list.append(features)  # Append the feature vector to the list\n",
    "\n",
    "        if oversample:\n",
    "            # Resample the features to match the maximum count (oversampling)\n",
    "            features_list = resample(features_list, replace=True, n_samples=max_count, random_state=42)\n",
    "        elif undersample:\n",
    "            # Resample the features to match the maximum count (undersampling)\n",
    "            features_list = resample(features_list, replace=False, n_samples=max_count, random_state=42)\n",
    "\n",
    "        all_features += features_list  # Append the features of the current genre to the overall list\n",
    "\n",
    "    # Return the numpy array of all extracted features along with corresponding genres\n",
    "    return np.array(all_features)\n",
    "\n",
    "\n",
    "# Call the extract_midi_features function with the appropriate path DataFrame to extract the MIDI \n",
    "# file features and obtain the feature-label matrix\n",
    "labeled_features = extract_midi_features(matched_midi_df)\n",
    "# Print the labeled features\n",
    "print(labeled_features)\n",
    "\n",
    "# Optionally store the feature-label matrix as a pickle file for further use\n",
    "with open('labeled_features_over.pkl', 'wb') as f:\n",
    "    pickle.dump(labeled_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3978\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# If saved, load matched_midi_df from the saved pickle file\n",
    "with open('labeled_features.pkl', 'rb') as f:\n",
    "    labeled_features = pickle.load(f)\n",
    "\n",
    "print(len(labeled_features))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4FRw4KCbWc17"
   },
   "source": [
    "## Partition Dataset into Training, Validation, and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ThBgMAJ1Wc17",
    "outputId": "04df37d3-562f-445f-b7e8-af9181f2fd26",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.63142102e-02 -3.50000000e-01  1.25000000e-01  1.25000000e-01\n",
      "   1.20000000e+00  3.28393136e-03  2.90862843e+04  3.84259595e+02\n",
      "   7.67716633e+03  5.93171646e+03  5.13025432e+03  1.35359018e+04\n",
      "   8.74087583e+02  2.56013226e+04  1.45344695e+02  3.38658483e+03\n",
      "   7.71258439e+03]\n",
      " [ 2.50000000e-01 -3.50000000e-01  1.25000000e-01  1.25000000e-01\n",
      "   8.00000000e-01  2.39785319e-03  2.35381800e+04  0.00000000e+00\n",
      "   3.40029800e+04  0.00000000e+00  8.49122000e+03  2.05917800e+04\n",
      "   0.00000000e+00  2.37896400e+04  0.00000000e+00  1.65608000e+04\n",
      "   2.03200000e+04]\n",
      " [ 9.21938089e-02 -5.30000000e-01  1.25000000e-01  1.25000000e-01\n",
      "   1.00000000e+00  5.29978587e-03  4.31800000e+01  1.71450000e+03\n",
      "   4.65201000e+03  2.66408545e+01  1.15529311e+04  1.93380444e+01\n",
      "   2.88290000e+02  0.00000000e+00  2.24155000e+03  1.46670957e+04\n",
      "   1.45424116e+03]\n",
      " [ 1.20149481e-01 -4.10000000e-01  1.25000000e-01  1.25000000e-01\n",
      "   1.20000000e+00  3.81565907e-03  1.60582000e+03  4.25184000e+03\n",
      "   4.15736000e+03  1.92419000e+03  5.95811000e+03  6.38640000e+02\n",
      "   1.00569900e+04  5.08874000e+03  1.31816000e+03  8.64190000e+02\n",
      "   1.65428000e+03]\n",
      " [ 6.66669500e-02  5.50000000e-01  1.25000000e-01  1.25000000e-01\n",
      "   8.00000000e-01  2.64336107e-03  2.45857519e+04  6.27287500e+01\n",
      "   7.92175313e+03  2.01603656e+04  9.93343750e+01  1.71605031e+04\n",
      "   3.82968750e+01  2.26822000e+04  1.05478500e+04  0.00000000e+00\n",
      "   8.63645000e+03]\n",
      " [ 2.30087196e-01  5.50000000e-01  1.25000000e-01  1.25000000e-01\n",
      "   8.00000000e-01  4.16666667e-03  2.99820000e+03  6.62530000e+03\n",
      "   0.00000000e+00  1.70724500e+04  0.00000000e+00  1.73741400e+04\n",
      "   1.08414000e+04  0.00000000e+00  1.23291000e+04  1.50100000e+02\n",
      "   4.43683900e+04]\n",
      " [ 3.62577745e-01 -3.50000000e-01  1.25000000e-01  1.25000000e-01\n",
      "   1.00000000e+00  3.41834927e-03  1.59656250e+02  4.06565375e+03\n",
      "   1.45251594e+03  9.46891562e+02  1.49664925e+04  6.18750000e+00\n",
      "   3.19852250e+03  8.64260000e+02  8.88536000e+03  7.93051250e+03\n",
      "   6.98250000e+00]\n",
      " [ 1.91214160e-01  3.10000000e-01  1.25000000e-01  1.25000000e-01\n",
      "   1.20000000e+00  3.44827586e-03  1.01862000e+03  3.68106421e+04\n",
      "   5.67179419e+01  7.28654000e+03  1.50084474e+04  5.78708789e+03\n",
      "   1.69603781e+03  1.80152737e+02  2.83914142e+04  1.21180181e+02\n",
      "   2.62445297e+03]\n",
      " [-4.13231405e-03 -3.50000000e-01  1.25000000e-01  1.25000000e-01\n",
      "   1.20000000e+00  2.56379962e-03  1.20411300e+04  2.21587100e+04\n",
      "   9.54507000e+03  1.07090700e+04  4.25533500e+04  3.50870000e+02\n",
      "   1.59863800e+04  1.32742000e+03  2.33781200e+04  2.28884500e+04\n",
      "   1.19667000e+03]\n",
      " [ 2.91156452e-01  3.10000000e-01  1.25000000e-01  1.25000000e-01\n",
      "   1.20000000e+00  4.54021970e-03  6.74207859e+03  2.78444975e+04\n",
      "   5.46950000e+02  2.07570887e+04  2.73724016e+03  3.45508311e+04\n",
      "   8.02042000e+03  6.66779000e+03  1.17717650e+04  1.67501297e+03\n",
      "   5.39749714e+04]]\n",
      "[4 3 3 6 3 3 3 3 8 3]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Shuffle the features\n",
    "labeled_features = np.random.permutation(labeled_features)\n",
    "\n",
    "# Partition the Dataset into 3 Sets: Training, Validation, and Test\n",
    "num = len(labeled_features)\n",
    "# Calculate the number of samples for training data (60% of the dataset)\n",
    "num_training = int(num * 0.6)\n",
    "# Calculate the number of samples for validation data (20% of the dataset)\n",
    "num_validation = int(num * 0.8)\n",
    "\n",
    "# Extract the training data (60% of the labeled features)\n",
    "training_data = labeled_features[:num_training]\n",
    "# Extract the validation data (20% of the labeled features)\n",
    "validation_data = labeled_features[num_training:num_validation]\n",
    "# Extract the test data (remaining 20% of the labeled features)\n",
    "test_data = labeled_features[num_validation:]\n",
    "\n",
    "\n",
    "# Separate the features from the labels\n",
    "num_cols = training_data.shape[1] - 1\n",
    "# Extract features from the training data\n",
    "training_features = training_data[:, :num_cols]\n",
    "# Extract features from the validation data\n",
    "validation_features = validation_data[:, :num_cols]\n",
    "# Extract features from the test data\n",
    "test_features = test_data[:, :num_cols]\n",
    "\n",
    "# Format the features for this multi-class classification problem\n",
    "num_classes = len(label_list)\n",
    "# Extract labels from the training data and convert them to integers\n",
    "training_labels = training_data[:, num_cols].astype(int)\n",
    "# Extract labels from the validation data and convert them to integers\n",
    "validation_labels = validation_data[:, num_cols].astype(int)\n",
    "# Extract labels from the test data and convert them to integers\n",
    "test_labels = test_data[:, num_cols].astype(int)\n",
    "\n",
    "print(test_features[:10])  # Print the first 10 rows of test features\n",
    "print(test_labels[:10])  # Print the first 10 test labels\n",
    "print(to_categorical((test_labels)[:10]))  # Print the one-hot encoding of the first 10 test labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Folk', 'Country', 'Pop_Rock', 'International', 'Vocal', 'RnB', 'New Age', 'Blues', 'Latin', 'Jazz', 'Reggae', 'Rap', 'Electronic']\n",
      "3978 [25, 2813, 13, 287, 98, 319, 118, 48, 148, 27, 11, 10, 61]\n"
     ]
    }
   ],
   "source": [
    "# print(test_labels)\n",
    "num_labels = 0\n",
    "num_genres = [0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "for i in labeled_features[:, num_cols].astype(int):\n",
    "    num_labels += 1\n",
    "    num_genres[i] += 1\n",
    "\n",
    "print(label_list)\n",
    "print (num_labels, num_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "I6jbF2S2ZKWi",
    "outputId": "f8f680a5-2652-4618-bd54-314ff3396e0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               4608      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 13)                845       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,605\n",
      "Trainable params: 46,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "75/75 - 2s - loss: 1.5641 - accuracy: 0.6199 - val_loss: 1.1973 - val_accuracy: 0.7048 - 2s/epoch - 24ms/step\n",
      "Epoch 2/50\n",
      "75/75 - 0s - loss: 1.3183 - accuracy: 0.7008 - val_loss: 1.1985 - val_accuracy: 0.7048 - 164ms/epoch - 2ms/step\n",
      "Epoch 3/50\n",
      "75/75 - 0s - loss: 1.2952 - accuracy: 0.7049 - val_loss: 1.1869 - val_accuracy: 0.7048 - 240ms/epoch - 3ms/step\n",
      "Epoch 4/50\n",
      "75/75 - 0s - loss: 1.2830 - accuracy: 0.7070 - val_loss: 1.1843 - val_accuracy: 0.7048 - 231ms/epoch - 3ms/step\n",
      "Epoch 5/50\n",
      "75/75 - 0s - loss: 1.2652 - accuracy: 0.7079 - val_loss: 1.1833 - val_accuracy: 0.7048 - 210ms/epoch - 3ms/step\n",
      "Epoch 6/50\n",
      "75/75 - 0s - loss: 1.2619 - accuracy: 0.7087 - val_loss: 1.1773 - val_accuracy: 0.7048 - 248ms/epoch - 3ms/step\n",
      "Epoch 7/50\n",
      "75/75 - 0s - loss: 1.2449 - accuracy: 0.7087 - val_loss: 1.1771 - val_accuracy: 0.7048 - 223ms/epoch - 3ms/step\n",
      "Epoch 8/50\n",
      "75/75 - 0s - loss: 1.2471 - accuracy: 0.7087 - val_loss: 1.1789 - val_accuracy: 0.7048 - 151ms/epoch - 2ms/step\n",
      "Epoch 9/50\n",
      "75/75 - 0s - loss: 1.2415 - accuracy: 0.7087 - val_loss: 1.1748 - val_accuracy: 0.7048 - 179ms/epoch - 2ms/step\n",
      "Epoch 10/50\n",
      "75/75 - 0s - loss: 1.2164 - accuracy: 0.7087 - val_loss: 1.1744 - val_accuracy: 0.7048 - 200ms/epoch - 3ms/step\n",
      "Epoch 11/50\n",
      "75/75 - 0s - loss: 1.2310 - accuracy: 0.7087 - val_loss: 1.1761 - val_accuracy: 0.7048 - 171ms/epoch - 2ms/step\n",
      "Epoch 12/50\n",
      "75/75 - 0s - loss: 1.2280 - accuracy: 0.7087 - val_loss: 1.1732 - val_accuracy: 0.7048 - 190ms/epoch - 3ms/step\n",
      "Epoch 13/50\n",
      "75/75 - 0s - loss: 1.2105 - accuracy: 0.7087 - val_loss: 1.1711 - val_accuracy: 0.7048 - 173ms/epoch - 2ms/step\n",
      "Epoch 14/50\n",
      "75/75 - 0s - loss: 1.2249 - accuracy: 0.7087 - val_loss: 1.1731 - val_accuracy: 0.7048 - 172ms/epoch - 2ms/step\n",
      "Epoch 15/50\n",
      "75/75 - 0s - loss: 1.2168 - accuracy: 0.7087 - val_loss: 1.1711 - val_accuracy: 0.7048 - 167ms/epoch - 2ms/step\n",
      "Epoch 16/50\n",
      "75/75 - 0s - loss: 1.2121 - accuracy: 0.7087 - val_loss: 1.1748 - val_accuracy: 0.7048 - 177ms/epoch - 2ms/step\n",
      "Epoch 17/50\n",
      "75/75 - 0s - loss: 1.2164 - accuracy: 0.7087 - val_loss: 1.1686 - val_accuracy: 0.7048 - 197ms/epoch - 3ms/step\n",
      "Epoch 18/50\n",
      "75/75 - 0s - loss: 1.2168 - accuracy: 0.7087 - val_loss: 1.1697 - val_accuracy: 0.7048 - 175ms/epoch - 2ms/step\n",
      "Epoch 19/50\n",
      "75/75 - 0s - loss: 1.2049 - accuracy: 0.7087 - val_loss: 1.1701 - val_accuracy: 0.7048 - 154ms/epoch - 2ms/step\n",
      "Epoch 20/50\n",
      "75/75 - 0s - loss: 1.2127 - accuracy: 0.7087 - val_loss: 1.1696 - val_accuracy: 0.7048 - 170ms/epoch - 2ms/step\n",
      "Epoch 21/50\n",
      "75/75 - 0s - loss: 1.2074 - accuracy: 0.7087 - val_loss: 1.1681 - val_accuracy: 0.7048 - 201ms/epoch - 3ms/step\n",
      "Epoch 22/50\n",
      "75/75 - 0s - loss: 1.2024 - accuracy: 0.7087 - val_loss: 1.1669 - val_accuracy: 0.7048 - 197ms/epoch - 3ms/step\n",
      "Epoch 23/50\n",
      "75/75 - 0s - loss: 1.2110 - accuracy: 0.7087 - val_loss: 1.1660 - val_accuracy: 0.7048 - 194ms/epoch - 3ms/step\n",
      "Epoch 24/50\n",
      "75/75 - 0s - loss: 1.1991 - accuracy: 0.7087 - val_loss: 1.1659 - val_accuracy: 0.7048 - 184ms/epoch - 2ms/step\n",
      "Epoch 25/50\n",
      "75/75 - 0s - loss: 1.2131 - accuracy: 0.7087 - val_loss: 1.1656 - val_accuracy: 0.7048 - 169ms/epoch - 2ms/step\n",
      "Epoch 26/50\n",
      "75/75 - 0s - loss: 1.2008 - accuracy: 0.7087 - val_loss: 1.1652 - val_accuracy: 0.7048 - 184ms/epoch - 2ms/step\n",
      "Epoch 27/50\n",
      "75/75 - 0s - loss: 1.2129 - accuracy: 0.7087 - val_loss: 1.1620 - val_accuracy: 0.7048 - 192ms/epoch - 3ms/step\n",
      "Epoch 28/50\n",
      "75/75 - 0s - loss: 1.2039 - accuracy: 0.7087 - val_loss: 1.1628 - val_accuracy: 0.7048 - 173ms/epoch - 2ms/step\n",
      "Epoch 29/50\n",
      "75/75 - 0s - loss: 1.2020 - accuracy: 0.7087 - val_loss: 1.1608 - val_accuracy: 0.7048 - 187ms/epoch - 2ms/step\n",
      "Epoch 30/50\n",
      "75/75 - 0s - loss: 1.1941 - accuracy: 0.7087 - val_loss: 1.1587 - val_accuracy: 0.7048 - 170ms/epoch - 2ms/step\n",
      "Epoch 31/50\n",
      "75/75 - 0s - loss: 1.1929 - accuracy: 0.7087 - val_loss: 1.1594 - val_accuracy: 0.7048 - 165ms/epoch - 2ms/step\n",
      "Epoch 32/50\n",
      "75/75 - 0s - loss: 1.1944 - accuracy: 0.7087 - val_loss: 1.1594 - val_accuracy: 0.7048 - 169ms/epoch - 2ms/step\n",
      "Epoch 33/50\n",
      "75/75 - 0s - loss: 1.1994 - accuracy: 0.7087 - val_loss: 1.1583 - val_accuracy: 0.7048 - 201ms/epoch - 3ms/step\n",
      "Epoch 34/50\n",
      "75/75 - 0s - loss: 1.1992 - accuracy: 0.7087 - val_loss: 1.1616 - val_accuracy: 0.7048 - 179ms/epoch - 2ms/step\n",
      "Epoch 35/50\n",
      "75/75 - 0s - loss: 1.1898 - accuracy: 0.7087 - val_loss: 1.1601 - val_accuracy: 0.7048 - 155ms/epoch - 2ms/step\n",
      "Epoch 36/50\n",
      "75/75 - 0s - loss: 1.1912 - accuracy: 0.7087 - val_loss: 1.1626 - val_accuracy: 0.7048 - 166ms/epoch - 2ms/step\n",
      "Epoch 37/50\n",
      "75/75 - 0s - loss: 1.1935 - accuracy: 0.7087 - val_loss: 1.1564 - val_accuracy: 0.7048 - 193ms/epoch - 3ms/step\n",
      "Epoch 38/50\n",
      "75/75 - 0s - loss: 1.1950 - accuracy: 0.7087 - val_loss: 1.1559 - val_accuracy: 0.7048 - 216ms/epoch - 3ms/step\n",
      "Epoch 39/50\n",
      "75/75 - 0s - loss: 1.1828 - accuracy: 0.7091 - val_loss: 1.1541 - val_accuracy: 0.7048 - 186ms/epoch - 2ms/step\n",
      "Epoch 40/50\n",
      "75/75 - 0s - loss: 1.1858 - accuracy: 0.7091 - val_loss: 1.1532 - val_accuracy: 0.7048 - 181ms/epoch - 2ms/step\n",
      "Epoch 41/50\n",
      "75/75 - 0s - loss: 1.1909 - accuracy: 0.7091 - val_loss: 1.1555 - val_accuracy: 0.7048 - 153ms/epoch - 2ms/step\n",
      "Epoch 42/50\n",
      "75/75 - 0s - loss: 1.1883 - accuracy: 0.7087 - val_loss: 1.1513 - val_accuracy: 0.7048 - 222ms/epoch - 3ms/step\n",
      "Epoch 43/50\n",
      "75/75 - 0s - loss: 1.1769 - accuracy: 0.7100 - val_loss: 1.1482 - val_accuracy: 0.7048 - 228ms/epoch - 3ms/step\n",
      "Epoch 44/50\n",
      "75/75 - 0s - loss: 1.1919 - accuracy: 0.7091 - val_loss: 1.1506 - val_accuracy: 0.7048 - 189ms/epoch - 3ms/step\n",
      "Epoch 45/50\n",
      "75/75 - 0s - loss: 1.1694 - accuracy: 0.7096 - val_loss: 1.1501 - val_accuracy: 0.7048 - 162ms/epoch - 2ms/step\n",
      "Epoch 46/50\n",
      "75/75 - 0s - loss: 1.1776 - accuracy: 0.7083 - val_loss: 1.1470 - val_accuracy: 0.7060 - 169ms/epoch - 2ms/step\n",
      "Epoch 47/50\n",
      "75/75 - 0s - loss: 1.1815 - accuracy: 0.7096 - val_loss: 1.1468 - val_accuracy: 0.7048 - 181ms/epoch - 2ms/step\n",
      "Epoch 48/50\n",
      "75/75 - 0s - loss: 1.1794 - accuracy: 0.7100 - val_loss: 1.1487 - val_accuracy: 0.7060 - 177ms/epoch - 2ms/step\n",
      "Epoch 49/50\n",
      "75/75 - 0s - loss: 1.1830 - accuracy: 0.7100 - val_loss: 1.1446 - val_accuracy: 0.7060 - 200ms/epoch - 3ms/step\n",
      "Epoch 50/50\n",
      "75/75 - 0s - loss: 1.1814 - accuracy: 0.7104 - val_loss: 1.1454 - val_accuracy: 0.7060 - 171ms/epoch - 2ms/step\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_3\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dropout\n",
      "......vars\n",
      "...layers\\dropout_1\n",
      "......vars\n",
      "...layers\\dropout_2\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...metrics\\mean_metric_wrapper\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........2\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-06-01 14:53:19         2704\n",
      "metadata.json                                  2023-06-01 14:53:19           64\n",
      "variables.h5                                   2023-06-01 14:53:19       591416\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-06-01 14:53:18         2704\n",
      "metadata.json                                  2023-06-01 14:53:18           64\n",
      "variables.h5                                   2023-06-01 14:53:18       591416\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_3\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dropout\n",
      "......vars\n",
      "...layers\\dropout_1\n",
      "......vars\n",
      "...layers\\dropout_2\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...metrics\\mean_metric_wrapper\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........2\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "(1, 17)\n",
      "predictions: \n",
      "[[0.015 0.031 0.006 0.719 0.067 0.007 0.005 0.062 0.042 0.028 0.003 0.003\n",
      "  0.011]]\n",
      "25/25 [==============================] - 0s 979us/step - loss: 1.1728 - accuracy: 0.7048\n",
      "Test Loss: 1.1728110313415527\n",
      "Test Accuracy: 0.7047738432884216\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "from keras.utils import to_categorical\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.Sequential([\n",
    "    # First hidden layer with 256 units and ReLU activation\n",
    "    keras.layers.Dense(256, input_shape=(training_features.shape[1],), activation='sigmoid'), # try relu and sigmoid\n",
    "    keras.layers.Dropout(0.5),  # Dropout layer to prevent overfitting\n",
    "    \n",
    "    # Second hidden layer with 128 units and ReLU activation\n",
    "    keras.layers.Dense(128, activation='sigmoid'),\n",
    "    keras.layers.Dropout(0.5),  # Dropout layer to prevent overfitting\n",
    "    \n",
    "    # Third hidden layer with 64 units and ReLU activation\n",
    "    keras.layers.Dense(64, activation='sigmoid'),\n",
    "    keras.layers.Dropout(0.5),  # Dropout layer to prevent overfitting\n",
    "    \n",
    "    # Output layer with num_classes units and softmax activation for multi-class classification\n",
    "    keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Print a summary of the model architecture\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "optimizer=\"adam\": The optimizer algorithm to use during training. \n",
    "Adam optimizer is chosen, which is a popular optimization algorithm known for its efficiency.\n",
    "\n",
    "loss='categorical_crossentropy': The loss function used to measure the discrepancy between the \n",
    "predicted output and the true output labels. Categorical cross-entropy is suitable for\n",
    "multi-class classification tasks.\n",
    "\n",
    "metrics=['accuracy']: The metric(s) to be evaluated during training and testing. \n",
    "Accuracy is a commonly used metric to assess the model's performance.\n",
    "\"\"\"\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Encode the training and validation labels using one-hot encoding\n",
    "train_labels_encoded = to_categorical(training_labels)\n",
    "val_labels_encoded = to_categorical(validation_labels)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "training_features, train_labels_encoded: Input features and corresponding labels for model training.\n",
    "\n",
    "validation_features, val_labels_encoded: Validation set used to monitor the model's performance \n",
    "                                         during training.\n",
    "\n",
    "batch_size=32: Number of samples per gradient update. Training data is divided into batches, \n",
    "               and the model's weights are updated after each batch.\n",
    "\n",
    "epochs=50: Number of times the model will iterate over the entire training dataset.\n",
    "\n",
    "callbacks: EarlyStopping to stop training if the validation loss does not improve for a certain \n",
    "           number of epochs, and ModelCheckpoint to save the best model based on validation loss.\n",
    "\"\"\"\n",
    "history = model.fit(training_features, train_labels_encoded, \n",
    "                    validation_data=(validation_features, val_labels_encoded),\n",
    "                    batch_size=32, epochs=50, verbose=2,\n",
    "                    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n",
    "                               keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True)])\n",
    "\n",
    "# Save the entire model to an h5 file\n",
    "model.save(\"my_model.h5\")\n",
    "\n",
    "# Optionally save the entire model as a pickle file\n",
    "with open('my_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "\n",
    "# Optionally load the pickled model from file\n",
    "with open('my_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Use the loaded model for prediction\n",
    "preds = model.predict(test_features[:1])\n",
    "print(test_features[:1].shape)\n",
    "\n",
    "print(\"predictions: \")\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(preds)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_features, to_categorical(test_labels))\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('matched_midi.pkl', 'rb') as f:\n",
    "    matched_midi_df = pickle.load(f)\n",
    "\n",
    "with open('labeled_features.pkl', 'rb') as f:\n",
    "    labeled_features = pickle.load(f)\n",
    "\n",
    "# with open('my_model.pkl', 'rb') as f:\n",
    "#     model = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify A New MIDI File Using NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[0.008 0.581 0.004 0.154 0.037 0.075 0.031 0.015 0.048 0.01  0.003 0.003\n",
      "  0.031]]\n",
      "Country\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pretty_midi\n",
    "\n",
    "def normalize_features(features):\n",
    "    \"\"\"\n",
    "    Normalizes the features to the range [-1, 1].\n",
    "\n",
    "    Parameters:\n",
    "        features (list of float): The array of features.\n",
    "\n",
    "    Returns:\n",
    "        list of float: Normalized features.\n",
    "    \"\"\"\n",
    "    # Normalize each feature based on its specific range\n",
    "    tempo = (features[0] - 150) / 300\n",
    "    num_sig_changes = (features[1] - 2) / 10\n",
    "    resolution = (features[2] - 260) / 400\n",
    "    time_sig_1 = (features[3] - 3) / 8\n",
    "    time_sig_2 = (features[4] - 3) / 8\n",
    "    melody_complexity = (features[5] - 0) / 10\n",
    "    melody_range = (features[6] - 0) / 80\n",
    "\n",
    "    # Normalize pitch class histogram\n",
    "    pitch_class_hist = [((f - 0) / 100) for f in features[7:-1]]\n",
    "\n",
    "    # Return the normalized feature vector\n",
    "    return [tempo, resolution, time_sig_1, time_sig_2, melody_complexity, melody_range] + pitch_class_hist\n",
    "\n",
    "def get_features(path):\n",
    "    \"\"\"\n",
    "    Extracts specific features from a MIDI file given its path using the pretty_midi library.\n",
    "    Handle any potential errors with MIDI files appropriately.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): The path to the MIDI file.\n",
    "\n",
    "    Returns:\n",
    "        list of float: The extracted features.\n",
    "    \"\"\"\n",
    "    # Creates a PrettyMIDI object by loading the MIDI file specified by the given path.\n",
    "    file = pretty_midi.PrettyMIDI(path)\n",
    "    \n",
    "    # tempo: the estimated tempo of the audio file\n",
    "    tempo = file.estimate_tempo()\n",
    "\n",
    "    # num_sig_changes: the number of time signature changes in the audio file\n",
    "    num_sig_changes = len(file.time_signature_changes)\n",
    "\n",
    "    # resolution: the time resolution of the audio file (in ticks per beat)\n",
    "    resolution = file.resolution\n",
    "\n",
    "\n",
    "    # Extract time signature information\n",
    "    ts_changes = file.time_signature_changes\n",
    "    ts_1, ts_2 = 4, 4\n",
    "    if len(ts_changes) > 0:\n",
    "        ts_1 = ts_changes[0].numerator\n",
    "        ts_2 = ts_changes[0].denominator\n",
    "    \n",
    "    # Extract melody-related features\n",
    "    # melody: a pitch class histogram of the audio file\n",
    "    melody = file.get_pitch_class_histogram()\n",
    "    # melody_complexity: the number of unique pitch classes in the melody\n",
    "    melody_complexity = np.sum(melody > 0)\n",
    "    # melody_range: the range of pitch classes in the melody\n",
    "    melody_range = np.max(melody) - np.min(melody)\n",
    "    # OPTIONAL feature melody_contour: the temporal evolution of pitch content in the audio file\n",
    "    # melody_contour = librosa.feature.tempogram(y=file.fluidsynth(fs=16000), sr=16000, hop_length=512)\n",
    "    # melody_contour = np.mean(melody_contour, axis=0)\n",
    "    # chroma: a chroma representation of the audio file\n",
    "    chroma = file.get_chroma()\n",
    "    # pitch_class_hist: the sum of the chroma matrix along the pitch axis\n",
    "    pitch_class_hist = np.sum(chroma, axis=1)\n",
    "\n",
    "    return normalize_features([tempo, num_sig_changes, resolution, ts_1,\n",
    "                    ts_2, melody_complexity, melody_range] + list(pitch_class_hist)) # + list(melody_contour))\n",
    "    \n",
    "midi_path = \"test.mid\"\n",
    "midi_features = np.asarray(get_features(midi_path))\n",
    "midi_features = np.expand_dims(midi_features, axis = 0)\n",
    "\n",
    "prediction = model.predict(midi_features)\n",
    "\n",
    "print(prediction)\n",
    "\n",
    "genre = np.argmax(prediction)\n",
    "print(label_list[genre])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
