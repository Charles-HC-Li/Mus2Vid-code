{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'DEAM_CQT_Dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_printoptions(sci_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mDEAM_CQT_Dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEAM_CQT_Dataset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mLSTM_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM_model\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'DEAM_CQT_Dataset'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "from DEAM_CQT_Dataset import DEAM_CQT_Dataset\n",
    "from LSTM_model import LSTM_model\n",
    "import numpy as np\n",
    "import librosa\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "annot_path = \"deam_dataset/DEAM_Annotations/annotations/annotations averaged per song/dynamic (per second annotations)/arousal.csv\"\n",
    "audio_path = \"deam_dataset/DEAM_audio/MEMD_audio/\"\n",
    "transform_path = \"transforms/\"\n",
    "transform_name = \"testing\"\n",
    "train_dataset = DEAM_CQT_Dataset(annot_path=annot_path, audio_path=audio_path, save_files=True, transform_path=transform_path, transform_name=transform_name, train=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=1, shuffle=True)\n",
    "print(\"Dataset length:\", train_dataset.__len__())\n",
    "\n",
    "model = LSTM_model(input_size=12, hidden_size=30, num_layers=60, out_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 60, 12])\n",
      "torch.Size([1, 60])\n",
      "torch.Size([1, 60])\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret 'torch.float64' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# print(data)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# print(target)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspecshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/m2v_cqt/DEAM_CQT_Dataset.py:88\u001b[0m, in \u001b[0;36mDEAM_CQT_Dataset.specshow\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     86\u001b[0m chroma_cq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_transform(index, save_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_files)\n\u001b[1;32m     87\u001b[0m HOP_LENGTH \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(sr\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 88\u001b[0m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspecshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchroma_cq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHOP_LENGTH\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/m2v_cqt_training/lib/python3.12/site-packages/librosa/display.py:1173\u001b[0m, in \u001b[0;36mspecshow\u001b[0;34m(data, x_coords, y_coords, x_axis, y_axis, sr, hop_length, n_fft, win_length, fmin, fmax, tuning, bins_per_octave, key, Sa, mela, thaat, auto_aspect, htk, unicode, intervals, unison, ax, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspecshow\u001b[39m(\n\u001b[1;32m    925\u001b[0m     data: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    950\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m QuadMesh:\n\u001b[1;32m    951\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display a spectrogram/chromagram/cqt/etc.\u001b[39;00m\n\u001b[1;32m    952\u001b[0m \n\u001b[1;32m    953\u001b[0m \u001b[38;5;124;03m    For a detailed overview of this function, see :ref:`sphx_glr_auto_examples_plot_display.py`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;124;03m    >>> fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43missubdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplexfloating\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1174\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1175\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to display complex-valued input. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShowing magnitude instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1176\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1177\u001b[0m         )\n\u001b[1;32m   1178\u001b[0m         data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/m2v_cqt_training/lib/python3.12/site-packages/numpy/core/numerictypes.py:417\u001b[0m, in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03mReturns True if first argument is a typecode lower/equal in type hierarchy.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m \n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issubclass_(arg1, generic):\n\u001b[0;32m--> 417\u001b[0m     arg1 \u001b[38;5;241m=\u001b[39m \u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issubclass_(arg2, generic):\n\u001b[1;32m    419\u001b[0m     arg2 \u001b[38;5;241m=\u001b[39m dtype(arg2)\u001b[38;5;241m.\u001b[39mtype\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret 'torch.float64' as a data type"
     ]
    }
   ],
   "source": [
    "(data, target) = train_dataset.__getitem__(320)\n",
    "data = data.reshape((1, *data.shape))\n",
    "target = target.reshape((1, *target.shape))\n",
    "print(data.shape)\n",
    "print(target.shape)\n",
    "output = model(data)        \n",
    "print(output.shape)\n",
    "print()\n",
    "# print(data)\n",
    "# print(target)\n",
    "train_dataset.specshow(index=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2v_cqt_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
